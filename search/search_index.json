{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Essential AI","text":"<p>Learn AI, the math way!</p> <p>Essential AI began as a repository for capturing crucial concepts in this ever-evolving field of AI. At Essential AI, I try and demystify the complexities surrounding Machine Learning and make it accessible to all. I believe that understanding the underlying mathematics is the key to unlocking the true beauty of Machine Learning and Artificial Intelligence.</p>"},{"location":"#website-contents","title":"Website Contents","text":""},{"location":"about/","title":"About me","text":"<p>Naresh Kumar Devulapally</p> <p>LinkedIn | Github | CV</p> <p>I am an MS Thesis candidate co-advised by Dr. Junsong Yuan and Dr. Sreyasee Das Bhattacharjee at The Visual Computing Lab in the Department of Computer Science at University at Buffalo, SUNY. My Research is focused on Computer Vision and Multimodal AI (Vision-Language models) with applications towards Emotion Recognition, Crowd Analytics, Transformer based Object Understanding, and Emotion Generation. My recent (first-author) works are accepted at ACM Multimedia 2023 (\u223c25% acceptance rate) and BigMM 2023.</p> <p>Research Updates</p> <p> Fall 2023:</p> <ul> <li>One Paper accepted at BigMM 2023.</li> <li>M.S. Thesis Candidate co-advised by Prof. Junsong Yuan and Prof. Sreyasee Das Bhattacharjee.</li> <li>Graduate Teaching Assistant @ (CSE 574) Machine Learning course at UB.</li> </ul> <p>Summer 2023:</p> <ul> <li>One Paper accepted at ACM Multimedia 2023.</li> <li>Graduate Teaching Assistant @ (CSE 701) Sport Analysis using Computer Vision seminar.</li> </ul> <p>Spring 2023:</p> <ul> <li>Submitted two papers to ACM Multimedia 2023.</li> <li>Graduate Teaching Asssistant @ (CSE 560) Data Models and Query Languages course at UB.</li> </ul> <p>Fall 2022:</p> <ul> <li>Supervised Researcher under the guidance of Professor Sreyasee Das Bhattacharjee and Professor Junsong Yuan. Submitted one paper to IJCAI 2023.</li> <li>Graduate Teaching Asssistant @ (CSE 555) Introduction to Pattern Recognition at UB.</li> </ul>"},{"location":"about/#research-projects","title":"Research Projects","text":"<p>Multi-label Emotion Analysis in Conversation via Multimodal Knowledge Distillation.</p> <p> </p> <p>PDF | Code | Poster | Web Page</p> <p><p> We present Self-supervised Multi-Label Peer Collaborative Distillation (SeMuL-PCD) Learning via an efficient Multimodal Transformer Network, in which complementary feedback from multiple mode-specific peer networks (e.g.transcript, audio, visual) are distilled into a single mode-ensembled fusion network for estimating multiple emotions simultaneously. </p></p> <p>AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations.</p> <p> </p> <p>PDF | Code | Poster | Web Page</p> <p><p> We propose a Multimodal Attention Network (MAN) that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode- specific Peripheral and Central networks. The proposed MAN \u201cinjects\u201d cross-modal attention via its Peripheral key-value pairs within each layer of a mode-specific Central query network. The resulting cross-attended mode-specific descriptors are then com- bined using an Adaptive Fusion (AF) technique that enables the model to integrate the  discriminative and complementary mode- specific data patterns within an instance-specific multimodal descriptor. </p></p> <p>Privacy-preserving Multi-modal Attentive Learning framework for real-time emotion tracking in conversations.</p> <p> </p> <p>PDF | Code | Poster | Web Page</p> <p><p> We propose a PRIvacy-preserving Multimodal Attentive Learning framework (PRIMAL) that derives the person independent normalized facial Action-Unit based features to estimate the participants\u2019 expression and keeps track of their spatio-temporal states and conversation dynamics in context of their surrounding environment to evaluate the speaker emotion. By designing a novel contrastive loss based optimization framework to capture the self- and cross-modal correlation within a learned descriptor, PRIMAL exhibits promise in accurately identifying the emotion state of an individual speaker in group conversations. </p></p>"},{"location":"about/#academic-projects","title":"Academic Projects","text":"<p>Projects done as a part of the courses at UB.</p>"},{"location":"test/","title":"Test","text":"<p>\\(\\mathbb{R}\\)</p> {     \"data\": [         {             \"x\": [                 \"giraffes\",                 \"orangutans\",                 \"monkeys\"             ],             \"y\": [                 20,                 14,                 23             ],             \"type\": \"bar\"         }     ] }"},{"location":"math-for-ml/","title":"Mathematics for Machine Learning and Deep Learning","text":"<p>This section contains mathematical algorithms, methods, theorems, and proofs that are borrowed by Machine Learning (ML) and Deep Learning (DL), or were intended to be general-purpose solutions and now find their use in ML and DL. </p> <p>You might be surprised to learn that many optimization methods used in ML/DL date back several decades in the field of mathematics. This section is particularly exciting as it helps us appreciate the fundamental essence of math in ML/DL.</p>"},{"location":"math-for-ml/gradient-descent/","title":"Gradient Descent","text":"<p>Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function.</p> <p>The algorithm for Gradient Descent was written by Louis Augustin Cauchy, A French Mathematician, in 1847 in his work, Methode generale pour la resolution des systemes d'equations simultanees (The term Machine Learning was coined in 1959 by Arthur Samuel). This means Gradient Descent is a general-purpose optimization algorithm and was adapted for Machine Learning.</p>"},{"location":"math-for-ml/gradient-descent/#what-is-gradient-descent","title":"What is Gradient Descent?","text":"<p>Given a multi-variable function \\(C(v_1, v_2, ... , v_j)\\), gradient descent solves the values of \\(v_1, v_2, ... , V_j\\) at which the value of the function \\(C(v)\\) is local minimum.</p> <p>Info</p> <p>Gradient Descent is an iterative algorithm (as you will see in a bit) and works best when the function \\(C(v)\\) is a convex function.</p> <p> Gradient Descent finds global minima for convex multi-variable functions.      Source </p> <p>Let \\(C(v)\\) be a fully convex quadratic function with two variables \\(v_1, v_2\\). Example of such functions can be:</p> \\[ C(v_1, v_2) = v_1^2+v_2^2 \\] <p>The plot for the function \\(C(v)\\) looks like:</p> <p> C(v) plot.      Source </p> <p>The variables to be solved here are \\(v_1\\) and \\(v_2\\) when the function \\(C(v)\\) is minimum. As we have a very simple function here with us, we can eye ball the minimum value of the function. In many scenarions, the function \\(C(v)\\) could look very complicated, yet convex.</p> <p>In Linear Algebra and calculus, a convex uni-variate function is minimum where the slope is \\(0\\). In the above function too, finding derivatives of the function w.r.t. the variables and equating them to zero is very simple. However, gradient descent is a general-purpose algoritm and is made to work with functions with many variables. (where optimization/differenentiation through plain calculus does not work).</p>"},{"location":"math-for-ml/gradient-descent/#the-intuition","title":"The Intuition","text":"<p>If we look closely, all convex multi-variate functions have a valley-like structure and this structure leads to the global minima. The intuition is that, if we imagine a ball at any point on the curve and roll it down the \"valley\" shape, it shall reach the global minima. This is bound to happen, as the function is convex and the local and global minima converge to the same point.</p>"},{"location":"math-for-ml/gradient-descent/#quick-questions","title":"Quick questions:","text":"<ol> <li>How do we initialize a point on the curve randomly?</li> <li>What is meant by the \"valley\" shape and how to move along the \"valley\" shape of the curve?</li> <li>When do we know if the minima is reached? Is there a hard limit on the number of steps (or the time for rolling)?</li> </ol>"},{"location":"math-for-ml/gradient-descent/#why-does-this-intuition-makes-sense-and-how-do-we-implement-it","title":"Why does this intuition makes sense and how do we implement it?","text":""},{"location":"multimodal-ai/","title":"Introduction","text":"<ul> <li> What is Multimodal AI?<ul> <li> What is meant by a modality?</li> </ul> </li> <li> How is information represented in a modality?</li> <li> Why is Multimodal AI important? Examples?</li> <li> What are examples of some multimodal tasks and dataset?</li> <li> Is it possible to build a unified multimodal framework to solve all multimodal tasks?</li> <li> What are some core challenges for multimodal tasks?</li> </ul>"},{"location":"nns-to-llms/","title":"From Neural Networks to Large Language Models","text":"<p>My notes to help me understand Neural Networks to LLMs from scratch.</p>"},{"location":"nns-to-llms/#to-do","title":"To-Do","text":"<ul> <li> What are Neural Networks?<ul> <li> The most basic decision making model.</li> <li> Activation Functions and their magic.</li> <li> The importance of weights and how to they influence.</li> </ul> </li> <li> Building learnable neural network from scratch.</li> <li> Understanding Text Representation and Feature Extraction<ul> <li> Bag of Words, TF-IDF, Word Embeddings (Word2Vec, GloVe):</li> <li> Understanding Word2Vec and its implementation</li> <li> Understanding Text Classification (Naive Bayes, SVM, Neural Networks)</li> </ul> </li> <li> Sequence learning models<ul> <li> Introduction to RNNs and LSTMs</li> </ul> </li> <li> Chapter on how attention was introduced in 2014 (way before 2017 paper).</li> </ul>"},{"location":"nns-to-llms/decision-making-and-nns/","title":"Basics of decision-making and modeling the human brain.","text":"<ol> <li>How does the human brain work? How exactly do we make decisions?</li> <li>What happens in the complex super computer that we carry in our heads that contains 100 billion neurons?</li> </ol> <p>Cracking these questions has been the holy grail of neuroscience that could (in-theory) pave way for the (seemingly) ultimate goal of mankind, creating an artificial brain with human-level cognition.</p> <p>While these potential applications seem fascinating, let's not get carried away. Let's ask more important pragmatic questions.</p> <ol> <li>How far have we come to UNDERSTAND the internal workings of the human brain?</li> <li>How far have we come to MODEL the internal workings of the human brain?</li> </ol> <p>I leave the first question to the best researchers in neuroscience. We shall come back to this question in later chapters.</p> <p>The second question is where the field of Artificial Intelligence really comes into picture. How exactly do we model the human brain (specifically the decision-making process) and how far have we come?</p> <p>Artificial Intelligence</p> <p>Artificial intelligence (AI) is a multidisciplinary field of computer science focused on creating machines and software capable of performing tasks that typically require human intelligence, such as learning, problem-solving, language understanding, and decision-making.</p>"},{"location":"nns-to-llms/decision-making-and-nns/#fundamental-information-unit-neuron","title":"Fundamental Information Unit (Neuron)","text":"<p>Neuroscience says that Neuron is the fundamental unit that is responsible for transferring information in the human brain. Early efforts in Artificial Intelligence concentrated around modeling such fundamental information units.</p> <p>How exactly do we model a neuron?</p> <p> Biological Neuron.      Source </p> <p>Don't be intimidated by the complexities in the above figure. I assure you, this is not a biology lecture. Let's shift our attention on important components.</p> <ol> <li>A Neuron takes input signals through dendrites.</li> <li>Trasnmits the information through Nucleus and Axon.</li> <li>Based on the activation of Neuron, the (processed) information is transmitted to other neurons through Axon Terminals.</li> </ol>"},{"location":"nns-to-llms/decision-making-and-nns/#ai-started-here","title":"AI started here!","text":"<p>Researchers in AI started modeling the human brain from here. We needed a unit that could:</p> <ol> <li>Take input information (like Dendrites).</li> <li> <p>Process the information (like Nucleus and Axon)(1).</p> <ol> <li>How exactly to process information, what does processing even mean? We will come back to this, soon.</li> </ol> </li> <li> <p>Transmit the information forward to other neurons.</p> </li> </ol> <pre><code>graph LR\n  A[Accept Input] --&gt; B[Process Information] --&gt; C{Activated?};\n  C--&gt;|Yes| D[Transmit activated result];\n  C ----&gt;|No| E[Transmit non-activated result];</code></pre> <p>Example</p> <p>Let's go through an example to make the requirement clearer.</p> <p>Let's assume you want to decide if you want to buy a new car or not? You are required to decide between Yes or No. Think about how exactly would one go about it?</p> <p>Before you start making a decision, you would first come up with some questions based on which we want to make a decision, such as:</p> <ol> <li>How often do I find myself needing a car?</li> <li>Am I financially capable of owing and maintaining a car?</li> <li>How many of my colleagues have a car? (Buying a car might be a symbol of prestige to you).</li> </ol> <p>Now based on all these questions you would make the decision of buying a car. However, you would not stop there. Let's assume having a car to travel to your work place is really important to you. In this case the first question would have more weight than the rest of the questions. Similarly, if you are financially incapable of affording a car, you would find it increasing difficult to decide in favor of purchasing a car.</p> <p>Info</p> <p>In other words, we formulate a set of binary inputs (also called features) with a weight associated to each feature representing the relative importance of each feature and based on the values of these features our decision is effected.</p> <p>Let's start representing things to make this concept clear. Let's assume we just have \\(3\\) features that we use to make a binary decision and each of these features is represented by \\(x_1\\), \\(x_2\\), and \\(x_3\\). Let the weights of these features be \\(w_1\\), \\(w_2\\), and \\(w_3\\), Hence the pictorial representation of the above decision-making model would look like:</p> <p>The above model is known as the Perceptron model (also known as an artificial neuron).</p> <p>How exactly do we go from inputs to these features to a decision output?</p> <p>All we have until now is:</p> <ol> <li>We have a bunch of binary inputs \\(x_1\\), \\(x_2\\), and \\(x_3\\).</li> <li>Each input comes with a corresponding weight \\(w_1\\), \\(w_2\\), and \\(w_3\\).</li> </ol> <p>The most plausible idea is to take a weighted sum of inputs to get an idea of the signal.</p> <p>For example, if the feature \\(x_1\\) matters to you more, then its accompanied by a larger positive weight \\(w_1\\) and vice-versa.</p> \\[ \\text{Weighted sum }= \\sum_j x_j w_j \\] <p>What after weighted sum? Let's say positive weights denote towards the decision \"YES\" and negative the decision \"NO\", if the weighted sum is very positive, you shall lean towards the decision \"YES\".</p> <p>This gives us a basic version of a decision-making model and the threshold is up to our discretion (We shall deal with this very shortly).</p> \\[ \\text{output } = \\left\\{ \\begin{matrix} \\text{Yes if } \\sum_j x_j w_j \\geq \\text{threshold}\\\\  \\text{No if } \\sum_j x_j w_j &lt; \\text{threshold}\\\\ \\end{matrix}\\right. \\] <p>There it is! We are getting closer to making a basic decision-making model. </p> <p>One neat trick is to incorporate this threshold into decision-making (named bias). The above equation changes to:</p> \\[ \\text{output } = \\left\\{ \\begin{matrix} \\text{Yes if } \\sum_j x_j w_j - \\text{threshold (b)} \\geq 0\\\\  \\text{No if } \\sum_j x_j w_j - \\text{threshold (b)} &lt; 0\\\\ \\end{matrix}\\right. \\] <p><code>b is replaced with -b in most books/reference material for simplicity.</code></p> <p>Perceptron</p> <ol> <li>Takes in binary inputs \\(x_1, ... x_i\\) and their corresponding weights \\(w_1, ... w_i\\).</li> <li>Computes the weighted sum for each input.</li> <li>Takes a binary decision based on a threshold.</li> </ol>"},{"location":"nns-to-llms/decision-making-and-nns/#but-how-does-the-perceptron-learn","title":"But how does the perceptron learn?","text":"\\[ \\text{output } = \\left\\{ \\begin{matrix} \\text{Yes if } \\sum_j x_j w_j + b \\geq 0\\\\  \\text{No if } \\sum_j x_j w_j + b &lt; 0\\\\ \\end{matrix}\\right. \\] <p>Take a hard look at the above equation. </p> <ol> <li> <p>What are the variables to be solved, above? (\\(x_i\\) represent the input features and the output is a decision. These are known to us. What we need are the optimal values of \\(w_i\\) and \\(b\\) that will help us make decision for unseen variables \\(x_k\\)).</p> </li> <li> <p>We need to come up with a framework, where we shall give a network (made of perceptrons) a bunch of examples (supervised learning) and network should come up with optimal values for weights and bias.</p> </li> </ol> <p>Perceptron</p> <p>There we have it. A very basic decision-making model. Although the model is far from complete, we have a model that takes in a bunch of binary inputs, calculates the weighted sum and gives out a prediction based on the inputs.</p> <p>Our next step is to make this model LEARN on it's own to find the optimal weights and make decisions.</p>"},{"location":"nns-to-llms/neural-networks/","title":"LEARNING to make decisions","text":"<p>In the previous section, we looked at a basic perceptron model that takes in binary inputs \\(x_i\\) and their corresponding weights \\(w_i\\) and makes a binary decision between <code>Yes</code> or <code>No</code> (labelled <code>0</code> or <code>1</code>).  </p> <p>However, we still cannot use the perceptron model because it is not trained. As the values for weights \\(w_i\\) and bias \\(b\\) are not optimal yet. This is exactly what we will discuss in this chapter. Making Neural Networks learn.</p>"},{"location":"nns-to-llms/neural-networks/#making-perceptron-learn","title":"Making Perceptron Learn","text":"<p>We have, with us a perceptron model, that makes binary decisions as follows:</p> <p>Using <code>0</code> for YES and <code>1</code> for NO.</p> \\[ \\text{output } = \\left\\{ \\begin{matrix} \\text{0 if } \\sum_j x_j w_j + b \\geq 0\\\\  \\text{1 if } \\sum_j x_j w_j + b &lt; 0\\\\ \\end{matrix}\\right. \\] <p>The two parameters, above that control the output of the model are <code>weights</code> and <code>bias</code>. Changing values of weights and biases changes the output of the model (perceptron). If we would like to make an optimized models, we have to learn the optimal values of <code>weights</code> and <code>biases</code> for a given problem at hand for a given dataset.</p> <p>In the above setting of perceptron we see that changing values of weights and bias can lead to any values on the real line output and with many features, slight changes in weights can lead to vast changes in the output, lets fix that using some function (called activation function) on top of the weighted output.</p> <p>Why Activation Function?</p> <ol> <li>Accept any input on the real number line (instead of just binary inputs).</li> <li>Make it possible that small changes to weights and bias would result in small changes to output so that we can come up with an algorithm to find the optimal values for \\(w_i\\) and \\(b\\).</li> <li>Scales the output of the perceptron between some values so that we can set an accept threshold along with the activation function.</li> </ol>"},{"location":"nns-to-llms/neural-networks/#sigmoid-activation","title":"Sigmoid Activation","text":"<p>Sigmoid (also called Logistic) function takes any input on the real line and returns value between 0 to 1. This works really well for our model as we can be sure that, no matter what the weights are, we know that the output of our model will be between 0 to 1. Importantly, this helps us to set a threshold of <code>0.5</code> to make decision. This is a very commonly accepted threshold in the field of Deep Learning.</p> \\[ \\text{sigmoid(z)} = \\sigma(z) = \\frac{1}{1+e^{-z}} \\] <p>In our case, the perceptron output becomes</p> \\[ \\text{output} = \\text{sigmoid} ( \\sum_j x_jw_j +b) \\] <p>Adding our threshold of <code>0.5</code> into the equation</p> \\[ \\text{output } = \\left\\{ \\begin{matrix} \\text{1 if } \\sigma(\\sum_j x_j w_j + b) \\geq 0.5\\\\  \\text{0 if } \\sigma(\\sum_j x_j w_j + b) &lt; 0.5\\\\ \\end{matrix}\\right. \\] <p>The sigmoid activation preserves our initial intuition that largely positive values lead to one decision and largely negative values lead to another decision. This still holds true after applying the sigmoid activation function as largely positive numbers shall be scaled closer to <code>1</code> and negative numbers are scaled towards <code>0</code>.</p> <p>But does using sigmoid function change the way perceptron is modeled?</p> <p>Let's take a look at the Sigmoid curve. To make sure that an inequality is maintained before and after a function is applied, there are some checks required to be passed.</p> <ol> <li>The function must be monotonically increasing or decreasing.</li> <li>The function must be defined everywhere on the real number line.</li> </ol> <p> </p> <p>From the above figure we see that sigmoid function satisfies these conditions, which means adding sigmoid function does not effect the perceptron decision making inequality.</p> <p>The crucial advantages of sigmoid activation:</p> <ol> <li> <p>It gives a continuous, smooth function which leads to an important details that small changes in weights \\(\\Delta w_j\\) and bias \\(\\Delta b\\) will produce a small change in the output \\(\\Delta \\text{output}\\). This detail is crucial that helps the sigmoid neuron (perceptron) learn.</p> </li> <li> <p>Sigmoid is differentiable throughout the number line. This is another crucial feature of sigmoid activation and shall be discussed in this chapter.</p> </li> </ol> \\[ \\Delta \\mbox{output} \\approx \\sum_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j} \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b, \\] <p>The above equation represents how the changes in output \\(\\Delta \\text{output}\\) is a linear function of changes in weights \\(\\Delta w_j\\) and changes in bias \\(\\Delta b\\).</p> <p>This is derived from the equation:</p> \\[ \\text{output} = \\text{sigmoid} ( \\sum_j x_jw_j +b) \\] <p>This linear dependence makes it easy for us to make small changes in weights and attain changes in output. This is what makes the learning possible. In other words, we are setting stage for our perceptron to learn.</p> <p>Is Sigmoid the only activation function that makes learning possible?</p> <p>Definitely not! Sigmoid activation function is one of the many Activation Functions present in the realm of Deep Learning. Even without the sigmoid activation, the perceptron can learn, however the learning would not be easier as changes in weights might drastically effect the output.</p> <p>Other Activation Functions:</p> <ol> <li>Softmax Activation</li> <li>ReLU Activation</li> <li>Leaky ReLU Activation</li> </ol> <p>and many more exist, but for now lets focus on the sigmoid function and make our first learning neural network.</p>"},{"location":"nns-to-llms/neural-networks/#we-have-sigmoid-neurons","title":"We have Sigmoid Neurons","text":"<ol> <li>We have a sigmoid neuron unit (a variant of perceptron) that takes in any inputs on number line and weights to make a binary decision.</li> <li> <p>Small changes to weights and bias makes small changes to output, this means we can come up with an algorithm to tune the values of \\(w_i\\) and \\(b\\) and find optimal values.(1)</p> <ol> <li>We still need a metric to denote what exactly optimal/good/bad means.</li> </ol> </li> <li> <p>Sigmoid activation also allows us to set a threshold of <code>0.5</code> to make decisions between two classes (<code>0</code> and <code>1</code>)</p> </li> </ol> <p> Why Sigmoid?     Source </p> <p>Sigmoid Neurons allows for the above architecture. We can stack up a bunch of sigmoid neurons as each of the neuron outputs a value between <code>0</code> and <code>1</code>. We can still use the threshold <code>0.5</code> for decision-making. This is wonderful for us because, all we need now is a learning algorithm to tweak the values of weights, and a metric to denote how good/bad output weights are for a given dataset.</p>"},{"location":"nns-to-llms/neural-networks/#neural-networks","title":"Neural Networks","text":"<p>Having understood the motivation and working of sigmoid neurons, we are now ready to understand a network of these artificial neurons as shown in the above figure, a Neural Network.</p> <p>A Neural Network is a layered combination of artificial neurons (any activation function can be used to the output neurons to make a decision) as shown in the figure below. Each node is an artificial neuron and is connected to other neurons present at various LAYERS.</p> <p> What is a Neural Network?     Source </p> <p>Why the network architecture?</p> <p>If you remember from the previous section, we modeled a perceptron based on the binary features we used to make a decision. In the initial case, these features are just simple Yes or No questions. However, not all features are simple and precise. There could be complex, abstract, interdependent features that lead to a decision. Hence we need network that could handle these cases.</p> <p>Terminology</p> <ol> <li>The first layer of a neural network like the above one is known as the input layer, as it takes inputs to the network (like the Dendrites in a biological neuron).</li> <li>The final layer that provides the output of a neural network is known as the output layer. This output layer could be connected to other networks. The output layer might contains one or more neurons (more on this later). For simplicity consider one output neuron that makes a binary decision. </li> <li>The layers that come after the input layer and before the final output layer are known as the hidden layers.</li> </ol> <p> Anatomy of a neural network     Source </p> <p>The above neural network is sometimes also known as a Multi-Layer Perceptron (MLP) or Feedforward Neural Networks as the information flow is in the left-to-right (forward) direction.</p>"},{"location":"nns-to-llms/neural-networks/#coding-feedforward-neural-networks-without-learning","title":"Coding FeedForward Neural Networks (without learning)","text":"<p> (Use Ctrl + click to open in new tab)</p> <p><pre><code>import numpy as np\n\nclass Perceptron:\n    def __init__(self):\n        self.weight = np.random.uniform(-1, 1) # 1 (To test the architecture)\n\n    def activate(self, value):\n        # Using the sigmoid activation function on every neuron\n        return 1 / (1 + np.exp(-value)) # value (To test code correctness)\n\nclass Layer:\n    def __init__(self, num_neurons):\n        self.neurons = [Perceptron() for _ in range(num_neurons)]\n\n    def feedforward(self, inputs):\n        neuron_values = []\n        for neuron in self.neurons:\n          value = 0\n          for i in inputs:\n            value+= i*neuron.weight\n          neuron_values.append(neuron.activate(value))\n        return neuron_values            \n\n\nclass NeuralNetwork:\n    def __init__(self, num_inputs, hidden_layers, num_outputs):\n        self.layers = []\n\n        # Input layer\n        self.layers.append(Layer(num_inputs))\n\n        # Hidden layers\n        for i in hidden_layers:\n            self.layers.append(Layer(i))\n\n        # Output layer\n        self.layers.append(Layer(num_outputs))\n\n    def feedforward(self, inputs):\n        for layer in self.layers[1:]: # we dont want to compute values again for inputs as we already have them.\n            inputs = layer.feedforward(inputs)\n        return inputs\n\n# Define the network\nnum_inputs = 4\nhidden_layers = [5, 4, 3, 2]\nnum_outputs = 1\n\nnetwork = NeuralNetwork(num_inputs, hidden_layers, num_outputs)\n\n# Input values\ninputs = [1, 2, 3, 4]\n\n# Get the output\noutput = network.feedforward(inputs)[0]\nprint(output)\nprint(1 if output&gt;=0.5 else 0)\n</code></pre> But where did the bias go? Every perceptron node comes with a corresponding bias as discussed before. However, most literature ignores bias (or incorporates this variable within the weight) as with an without bias the output of neuron after activation (sigmoid for example) falls within a range.</p> <p>Hence to remove extra learnable paramters, bias is incorporated into the weight itself and the challenge boils down to finding the optimal weights of each neuron.</p> <p>If you still want to explicitly maintain a bias term, the perceptron class and Layer class can be changed to</p> <pre><code>class Perceptron:\n    def __init__(self):\n        self.weight = np.random.uniform(-1, 1)\n        self.bias = np.random.uniform(-1, 1)  # Initialize bias\n\n    def activate(self, value):\n        return 1 / (1 + np.exp(-value))  # Sigmoid activation\n\n# Layer class\nvalue+= i*neuron.weight + neuron.bias\n</code></pre> <p>The above code implements a basic feedforward neural network without any learning component. This is a great place to start. All we want now is an algorithm to find the optimal values for weights to make decisions on unseen data points.</p>"},{"location":"nns-to-llms/neural-networks/#lets-make-a-neural-network-learn","title":"Let's make a Neural Network Learn.","text":"<ol> <li>We have a neural network that accepts inputs.</li> <li>Hidden layers calculate weighted sum to transmit information.</li> <li>We have an activation function for each neuron.</li> <li>Each neuron has a weight and the optimal values of these weights have to be learnt.</li> </ol> \\[ \\text{output } (y_i) = \\text{sigmoid}(\\sum_j x_jw_j+b) \\] <p>ANNs</p> <p>Artificial Neural Networks and Deep Learning models learn from a bunch of examples.</p>"},{"location":"nns-to-llms/neural-networks/#the-beauty-of-neural-networks","title":"The beauty of Neural Networks.","text":"<p>If you didn't realize, the neural networks are built in this way to facilitate learning from examples.</p> <ol> <li>First we define a task at hand (such as image recognition).</li> <li>We collect data for the task that has a bunch of data points and their corresponding ground truth output. Think in terms of math that we will have \\(x_i\\) and \\(\\text{output}\\) for each data point.</li> <li>We randomly initialize weights of each neuron in the network and make predictions (random predictions which are intially very wrong).</li> <li>We define a metric that helps us understand the correctness of neural network predictions.</li> <li>We use an algorithm that tunes the values of weights of neurons in neural network such that the correctness in prediction of our network is maximized. This algorithm is the learning process of the network (also known as training).</li> <li>Upon training, we will have the optimal values for all the weights \\(w_i\\). </li> <li>These weights are then used to predict values for unseen inputs using our trained model.</li> </ol>"},{"location":"nns-to-llms/neural-networks/#the-problem-and-the-dataset-setup","title":"The problem and the dataset setup","text":"<p>To train our first Neural Network we are going to use the classic MNIST dataset for digit recognition. Below is a snapshot of some images in the dataset.</p> <p> MNIST dataset example images.     Source </p> <p>The plan is to send a bunch of example data points (image information) and labels to the neural network and train the neural network to recognize and distinguish between numbers between 0 to 9.</p>"},{"location":"nns-to-llms/neural-networks/#why-not-a-rule-based-approach","title":"Why not a rule-based approach?","text":"<p>Think about the brute-force way one would solve digit recognition. In our minds we have specific representation of digits and with immense ease we can distinguish between digits very easily. However, that ease is deceptive. Image if we wanted to come up with specific rules to distinguish digits. </p> <p>The number of rules required to make such an algorithm would blow up very quickly as there could be numerous ways in which each digit can be written. Take a look at the below image for example.</p> <p> Rule based approach does not work as there could be numerous ways to represent every digit differently.     Source </p> <p>Hence, we need a learning algorithm that could figure out the underlying patterns within all these digits and distinguish between various digits automatically after training. This will be our first Learning Algorithm that we will implement from scratch.</p>"},{"location":"nns-to-llms/neural-networks/#the-plan","title":"The Plan","text":"<pre><code>graph LR\n  A(\"Gather Data\n   (Digit Images)\") --&gt; B[Pre-Process Data to feed\n   into Neural Network] --&gt; C[Devise a \n   Learning Algorithm]\n   --&gt; D[Initialize the \n   Neural Network] --&gt; E[Train using the \n   pre-processed dataset] --&gt; F(Predict on new data points \n   not present in training dataset);</code></pre>"},{"location":"nns-to-llms/neural-networks/#action-items","title":"Action Items","text":"<ul> <li> Gather data containing digit images and labels (MNIST).</li> <li> Pre-Process the dataset so that Neural Network (NN) can accept the data.</li> <li> Split the data into train and test. We want to save some data to test the model's performance.</li> <li> Build a learning algorithm that can be used by any NN to train on input data.</li> <li> Train on the pre-processed train dataset.</li> <li> Evaluate on the test dataset.</li> </ul>"},{"location":"nns-to-llms/neural-networks/#mnist-dataset","title":"MNIST dataset","text":"<p>There are numerous ways to get the MNIST dataset as its quite a famous dataset. The original dataset was release by Dr. Yann Lecun et. al., and can be downloaded here. (The link does not open on chromium based browsers, try safari or firefox browsers, but <code>wget</code> still works).</p> <p>MNIST dataset is a collection of 60,000 images of hand-written digits. Each image has a dimension of \\(28 \\times 28\\). How do we process this dataset? Every value in the image represents a pixel intensity. The way we modeled input layer of the neural network, it should be able accept any input on the number line.</p> <p>However, how to we process 2-dimensional data? One neat trick is to flatten the \\(28 \\times 28\\) into a 1-D vector of size \\(728\\). Then we can feed these \\(728\\) values into the input layer of the neural network. That's exactly what we are going to do.</p> <p><pre><code>import numpy as np\nfrom urllib import request\nimport gzip\nimport pickle\n\nfilename = [\n[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n]\n\ndef download_mnist():\n    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n    for name in filename:\n        print(\"Downloading \"+name[1]+\"...\")\n        request.urlretrieve(base_url+name[1], name[1])\n    print(\"Download complete.\")\n\ndef save_mnist():\n    mnist = {}\n    for name in filename[:2]:\n        with gzip.open(name[1], 'rb') as f:\n            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n    for name in filename[-2:]:\n        with gzip.open(name[1], 'rb') as f:\n            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n    with open(\"mnist.pkl\", 'wb') as f:\n        pickle.dump(mnist,f)\n    print(\"Save complete.\")\n\ndef init():\n    download_mnist()\n    save_mnist()\n\ndef load():\n    with open(\"mnist.pkl\",'rb') as f:\n        mnist = pickle.load(f)\n    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n\ninit()\nx_train, y_train, x_test, y_test = load()\n</code></pre> We have loaded the data in the form of numpy arrays into <code>x_train</code>, <code>y_train</code>, <code>x_test</code>, <code>y_test</code>. All of the train arrays are numpy arrays containing the pixel value of each image as vectors of size \\(728\\).</p> <p>If we plot the first image of the dataset, we see the following image</p> <pre><code>import matplotlib.pyplot as plt\n\nimg = x_train[0,:].reshape(28,28) # First image in the training set.\nplt.imshow(img,cmap='gray')\nplt.show() # Show the image\n</code></pre> <p></p>"},{"location":"nns-to-llms/neural-networks/#modifications-to-the-neural-network","title":"Modifications to the neural network","text":"<p>Until now, we had a neural network that makes a binary decision. How do we modify the network to make a multi-class classification.</p> <p>One way is to add as many number of neurons in the output layers as the number of classes present in the datset. This means the final layer for the MNIST digit classification network contains <code>10</code> output neurons as there are <code>10</code> classes <code>0, 1, 2, 3, 4, 5, 6, 7, 8, 9</code>. The number of hidden layers and nodes in each layers is upto us. The input and output layers are the ones that are required to be modified.</p> <p> A possible neural network to classify MNIST digits.     Source </p> <p>In the above network, we use one neuron per class. This means based on the neuron that is activated for a given image, we classify the input image. Of course, testing unseen images happens after the training is complete. During training we tune the weights of neurons in a way that representations, patterns and intricacies between images and labels are learnt by the network.</p> <p>What if more than one neuron in the output layer is activated for an image?</p> <p>In the above network setting, there is a possibility that for a given image, exmaple an image of 5, both 4th and 5th neurons in the output layers, which label do you assign. Softmax is yet another activation function that can be used in this case.</p> <p>Softmax Activation (for the final layer).</p> <p>Softmax function takes in an array of inputs and gives a probability measure for each input value. Using softmax activation we can take <code>argmax</code> of output values and the label with maximum probability is returned. (As I said before, there could many activation functions). (Sigmoid and Softmax are two of the many).</p> <ul> <li> Gather data containing digit images and labels (MNIST).</li> <li> Pre-Process the dataset so that Neural Network (NN) can accept the data.</li> <li> Split the data into train and test. We want to save some data to test the model's performance.</li> <li> Build a learning algorithm that can be used by any NN to train on input data.</li> <li> Train on the pre-processed train dataset.</li> <li> Evaluate on the test dataset.</li> </ul>"},{"location":"nns-to-llms/neural-networks/#learning-with-gradient-descent","title":"Learning with Gradient Descent","text":"<p>Now as we have the dataset and the Neural Network ready, the next step for us to think of a way to make the model learn.</p> <p>We will use the notation \\(X\\) to denote the training input. Each training input \\(X_i\\) is a \\(28 \\times 28 = 728\\) dimensional vector.</p>"}]}